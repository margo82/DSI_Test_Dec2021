{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "674e0aa4",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-02-28T12:21:15.724776Z",
     "iopub.status.busy": "2022-02-28T12:21:15.723886Z",
     "iopub.status.idle": "2022-02-28T12:21:16.470462Z",
     "shell.execute_reply": "2022-02-28T12:21:16.469781Z",
     "shell.execute_reply.started": "2022-02-28T09:33:39.976591Z"
    },
    "papermill": {
     "duration": 0.773377,
     "end_time": "2022-02-28T12:21:16.470620",
     "exception": false,
     "start_time": "2022-02-28T12:21:15.697243",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/__notebook__.ipynb\n",
      "/kaggle/input/ubiquant-market-prediction/example_sample_submission.csv\n",
      "/kaggle/input/ubiquant-market-prediction/example_test.csv\n",
      "/kaggle/input/ubiquant-market-prediction/train.csv\n",
      "/kaggle/input/ubiquant-market-prediction/ubiquant/competition.cpython-37m-x86_64-linux-gnu.so\n",
      "/kaggle/input/ubiquant-market-prediction/ubiquant/__init__.py\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/working'): \n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "        \n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9124fa52",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-28T12:21:16.501012Z",
     "iopub.status.busy": "2022-02-28T12:21:16.500125Z",
     "iopub.status.idle": "2022-02-28T12:21:16.503084Z",
     "shell.execute_reply": "2022-02-28T12:21:16.503626Z",
     "shell.execute_reply.started": "2022-02-28T09:33:40.881225Z"
    },
    "papermill": {
     "duration": 0.019302,
     "end_time": "2022-02-28T12:21:16.503785",
     "exception": false,
     "start_time": "2022-02-28T12:21:16.484483",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "filename = '/kaggle/input/ubiquant-market-prediction/train.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11b14567",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-28T12:21:16.532726Z",
     "iopub.status.busy": "2022-02-28T12:21:16.531845Z",
     "iopub.status.idle": "2022-02-28T12:29:04.402524Z",
     "shell.execute_reply": "2022-02-28T12:29:04.401982Z",
     "shell.execute_reply.started": "2022-02-28T09:33:40.887072Z"
    },
    "papermill": {
     "duration": 467.885859,
     "end_time": "2022-02-28T12:29:04.402694",
     "exception": false,
     "start_time": "2022-02-28T12:21:16.516835",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- METADATA ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3141410 entries, 0 to 3141409\n",
      "Data columns (total 4 columns):\n",
      " #   Column         Dtype  \n",
      "---  ------         -----  \n",
      " 0   row_id         object \n",
      " 1   time_id        int16  \n",
      " 2   investment_id  int16  \n",
      " 3   target         float16\n",
      "dtypes: float16(1), int16(2), object(1)\n",
      "memory usage: 212.3 MB\n",
      "(3141410, 4) None\n",
      "row_id            object\n",
      "time_id            int16\n",
      "investment_id      int16\n",
      "target           float16\n",
      "dtype: object\n",
      "\n",
      "--- Features dataset ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3141410 entries, 0 to 3141409\n",
      "Columns: 301 entries, time_id to f_299\n",
      "dtypes: float16(300), int16(1)\n",
      "memory usage: 1.8 GB\n",
      "(3141410, 301) None\n",
      "time_id      int16\n",
      "f_0        float16\n",
      "f_1        float16\n",
      "f_2        float16\n",
      "f_3        float16\n",
      "            ...   \n",
      "f_295      float16\n",
      "f_296      float16\n",
      "f_297      float16\n",
      "f_298      float16\n",
      "f_299      float16\n",
      "Length: 301, dtype: object\n"
     ]
    }
   ],
   "source": [
    "met=[]\n",
    "dat=[]\n",
    "\n",
    "dtypes = {'row_id':'category'\n",
    "          , 'time_id':'int16'\n",
    "          , 'investment_id':'int16'\n",
    "          , 'target': 'float16'}\n",
    "dtypes.update(dict.fromkeys([f'f_{i}' for i in range(300)],'float16'))\n",
    "\n",
    "for chunk in pd.read_csv(filename, chunksize=100000,na_values='nan',dtype = dtypes):\n",
    "    # load the metadata seperate from the features (ie variables)\n",
    "    #met.append(chunk.iloc[:,0:4])\n",
    "    dat.append(chunk)\n",
    "    #break  # break  here load less data and run some EDA - to be removed\n",
    "\n",
    "df = pd.concat(dat)\n",
    "\n",
    "met = df.iloc[:,0:4]  # other data than the predictive features (incl. the target variable)\n",
    "df = df.drop(['row_id','investment_id','target'], axis = 1)  #remove some variables but keep time_id for now\n",
    "\n",
    "print('--- METADATA ---')\n",
    "print(met.shape, met.info(memory_usage='deep'))\n",
    "print(met.dtypes)\n",
    "\n",
    "print('\\n--- Features dataset ---')\n",
    "print(df.shape, df.info(memory_usage='deep'))\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7ec0b83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-28T12:29:04.435058Z",
     "iopub.status.busy": "2022-02-28T12:29:04.434473Z",
     "iopub.status.idle": "2022-02-28T12:29:16.630925Z",
     "shell.execute_reply": "2022-02-28T12:29:16.630398Z",
     "shell.execute_reply.started": "2022-02-28T09:39:59.803425Z"
    },
    "papermill": {
     "duration": 12.214902,
     "end_time": "2022-02-28T12:29:16.631056",
     "exception": false,
     "start_time": "2022-02-28T12:29:04.416154",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6055, 301)\n"
     ]
    }
   ],
   "source": [
    "# the \"groupby\" craches the run So I went through a first step to select randomly datapoint within the dataset\n",
    "reduce_index = df.groupby('time_id').apply(lambda x: x.sample(5)).index  #.reset_index(drop=True)   # get the indeces here because I need to use it on target value too\n",
    "dfred = df.iloc[reduce_index.get_level_values(1)]\n",
    "print(dfred.shape)\n",
    "dfred.head()\n",
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82efa7b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-28T12:29:16.666260Z",
     "iopub.status.busy": "2022-02-28T12:29:16.665560Z",
     "iopub.status.idle": "2022-02-28T12:29:16.889642Z",
     "shell.execute_reply": "2022-02-28T12:29:16.890005Z",
     "shell.execute_reply.started": "2022-02-28T09:40:09.450036Z"
    },
    "papermill": {
     "duration": 0.244987,
     "end_time": "2022-02-28T12:29:16.890146",
     "exception": false,
     "start_time": "2022-02-28T12:29:16.645159",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6055, 300) (6055,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X = np.array(dfred.iloc[:,1:])\n",
    "Y = np.array(met.target.iloc[reduce_index.get_level_values(1)])\n",
    "\n",
    "scaler.fit(X)\n",
    "X = scaler.transform(X)\n",
    "\n",
    "print(np.shape(X), np.shape(Y))\n",
    "\n",
    "del dfred\n",
    " \n",
    "#??? do we need to know if it is needed or just goog practice\n",
    "#!!! Problem here is that we will run out of memory by creating new arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79bcd216",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-28T12:29:16.916473Z",
     "iopub.status.busy": "2022-02-28T12:29:16.915911Z",
     "iopub.status.idle": "2022-02-28T12:29:16.963398Z",
     "shell.execute_reply": "2022-02-28T12:29:16.964212Z",
     "shell.execute_reply.started": "2022-02-28T09:40:09.71146Z"
    },
    "papermill": {
     "duration": 0.064037,
     "end_time": "2022-02-28T12:29:16.964407",
     "exception": false,
     "start_time": "2022-02-28T12:29:16.900370",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4238, 300) (1817, 300) (4238,) (1817,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "del X\n",
    "del Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27ce5619",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-28T12:29:16.990653Z",
     "iopub.status.busy": "2022-02-28T12:29:16.990142Z",
     "iopub.status.idle": "2022-02-28T12:33:50.419670Z",
     "shell.execute_reply": "2022-02-28T12:33:50.420235Z",
     "shell.execute_reply.started": "2022-02-28T09:40:09.767275Z"
    },
    "papermill": {
     "duration": 273.44519,
     "end_time": "2022-02-28T12:33:50.420528",
     "exception": false,
     "start_time": "2022-02-28T12:29:16.975338",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confidence:  -0.03084117156967725\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf = RandomForestRegressor()\n",
    "rf.fit(x_train, y_train)   #fit the model\n",
    "rf_confidence = rf.score(x_test, y_test)  #test against the test dataset and give the coefficient of determination of the prediction (best score is 1 & score can be <0)\n",
    "print('confidence: ', rf_confidence)  #or accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec7e4aa6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-28T12:33:50.458512Z",
     "iopub.status.busy": "2022-02-28T12:33:50.457814Z",
     "iopub.status.idle": "2022-02-28T12:33:50.591437Z",
     "shell.execute_reply": "2022-02-28T12:33:50.591994Z",
     "shell.execute_reply.started": "2022-02-28T09:45:07.08941Z"
    },
    "papermill": {
     "duration": 0.156078,
     "end_time": "2022-02-28T12:33:50.592159",
     "exception": false,
     "start_time": "2022-02-28T12:33:50.436081",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This version of the API is not optimized and should not be used to estimate the runtime of your code on the hidden test set.\n"
     ]
    }
   ],
   "source": [
    "import ubiquant\n",
    "env = ubiquant.make_env()   # initialize the environment\n",
    "iter_test = env.iter_test()    # an iterator which loops over the test set and sample submission\n",
    "for (test_df, sample_prediction_df) in iter_test:\n",
    "    test_df.drop(['row_id','investment_id'], axis=1, inplace=True)\n",
    "    #print(test_df.shape)\n",
    "    sample_prediction_df['target'] = rf.predict(test_df)  # make your predictions here\n",
    "    env.predict(sample_prediction_df)   # register your predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598cdad2",
   "metadata": {
    "papermill": {
     "duration": 0.015782,
     "end_time": "2022-02-28T12:33:50.624243",
     "exception": false,
     "start_time": "2022-02-28T12:33:50.608461",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 763.12777,
   "end_time": "2022-02-28T12:33:51.853801",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-02-28T12:21:08.726031",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
